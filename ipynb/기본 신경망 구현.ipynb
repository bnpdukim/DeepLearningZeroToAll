{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기본 신경망 구현"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "인공신경망\n",
    "  - 뇌를 구성하는 신경세포(뉴런)의 동작 원리와 동일\n",
    "  - 그림 4-1\n",
    "  - 그림 4-2\n",
    "활성화 함수\n",
    "  - 인공신경망을 통과해온 값을 최종적으로 어떤 값으로 만들지 결정\n",
    "  - Sigmoid, ReLU, tanh\n",
    "  - 그림 4-4\n",
    "인공 신경망\n",
    "  - 가중치와 활성화 함수의 연결로 이루어져 있음\n",
    "  - 그림 4-5\n",
    "  - 심층 싱경망 : 그림 4-6\n",
    "역전파\n",
    "  - 출력층이 내놓은 결과의 오차를 신경망을 따라 입력층까지 역으로 계산\n",
    "  - 가중치를 최적화 과정이 훨씬 빠르고 정확함\n",
    "  - 그림 4-7\n",
    "  - 구현 어려우나 텐서플로우는 기본 제공\n",
    "분류(classification)\n",
    "  - 패턴을 파악해 여러 종류로 구분하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털유무, 날개유무]\n",
    "x_data = np.array([[0,0],[1,0],[1,1],[0,0],[0,0],[0,1]]) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "원-핫 인코딩(one-hot encoding)\n",
    "  - 데이터가 가질수 있는 값들중 표현하려고 하는 값만 1로 표현하고 나머지는 0으로 채우는 표기법\n",
    "  - 기타 =   [1,0,0]\n",
    "  - 포유류 = [0,1,0]\n",
    "  - 조류 =   [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data = np.array([\n",
    "    [1,0,0], # 기타\n",
    "    [0,1,0], # 포유류\n",
    "    [0,0,1], # 조류\n",
    "    [1,0,0],\n",
    "    [1,0,0],\n",
    "    [0,0,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# [털, 날개] -> [기타,포유류,조류]\n",
    "#   [0,0]    ->  [1,    0,    0]\n",
    "#   [1,0]    ->  [0,    1,    0]\n",
    "#   [1,1]    ->  [0,    0,    1]\n",
    "#   [0,0]    ->  [1,    0,    0]\n",
    "#   [0,0]    ->  [1,    0,    0]\n",
    "#   [0,1]    ->  [0,    0,    1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([2,3], -1., 1.))  # [입력층 특징 수, 출력층 레빌블 수]\n",
    "b = tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = tf.add( tf.matmul(X,W), b )\n",
    "L = tf.nn.relu(expr)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "그림 4-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tf.nn.softmax(L)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "softmax \n",
    "  - 배열 내의 결과값들을 전체 합이 1이 되도록 만들어 주는 함수\n",
    "  - ex> [8.04, 2.76, -6.52] -> [0.53, 0.24, 023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(model), axis = 1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "교차 엔트로피(Cross-Entropy)\n",
    "  - 예측값과 실제값 사이의 확률 분포 차이를 계산한 값\n",
    "  - 원-핫 인코딩을 이용한 대부분의 모델에서 사용    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.1739\n",
      "11 1.16349\n",
      "21 1.15334\n",
      "31 1.14344\n",
      "41 1.13378\n",
      "51 1.12438\n",
      "61 1.11523\n",
      "71 1.1063\n",
      "81 1.0976\n",
      "91 1.08912\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X:x_data, Y:y_data})\n",
    "    \n",
    "    if step%10 == 0:\n",
    "        print(step+1, sess.run(cost, feed_dict={X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값 :  [2 2 1 2 2 1]\n",
      "실제값 :  [0 1 2 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.argmax(model, axis = 1)\n",
    "target = tf.argmax(Y, axis=1)\n",
    "print('예측값 : ', sess.run(prediction, feed_dict = {X: x_data}))\n",
    "print('실제값 : ', sess.run(target, feed_dict={Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.00\n"
     ]
    }
   ],
   "source": [
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도 : %.2f'% sess.run(accuracy * 100, feed_dict={X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "정확도 높이는 방법\n",
    "  - 심층 신경망 활용\n",
    "  - 그림 4-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_uniform([2,10], -1., 1.))  # [특징, 은닉층의 뉴런 수]\n",
    "W2 = tf.Variable(tf.random_uniform([10,3], -1., 1.))  # [은닉층의 뉴런 수, 분류 수]\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([10])) # 은닉층의  뉴런 수\n",
    "b2 = tf.Variable(tf.zeros([3])) # 분류 수\n",
    "\n",
    "expr1 = tf.add(tf.matmul(X,W1), b1)\n",
    "L1 = tf.nn.relu(expr1)\n",
    "\n",
    "model = tf.add(tf.matmul(L1, W2), b2)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "은닉층의 뉴런 수는 실험을 통해 가장 적절한 수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.22164\n",
      "11 1.07292\n",
      "21 0.963965\n",
      "31 0.857944\n",
      "41 0.757344\n",
      "51 0.654823\n",
      "61 0.554788\n",
      "71 0.4613\n",
      "81 0.378268\n",
      "91 0.308292\n",
      "예측값 :  [0 1 2 0 0 2]\n",
      "실제값 :  [0 1 2 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X:x_data, Y:y_data})\n",
    "    \n",
    "    if step%10 == 0:\n",
    "        print(step+1, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "prediction = tf.argmax(model, axis = 1)\n",
    "target = tf.argmax(Y, axis=1)\n",
    "print('예측값 : ', sess.run(prediction, feed_dict = {X: x_data}))\n",
    "print('실제값 : ', sess.run(target, feed_dict={Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 100.00\n"
     ]
    }
   ],
   "source": [
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도 : %.2f'% sess.run(accuracy * 100, feed_dict={X:x_data, Y:y_data}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
